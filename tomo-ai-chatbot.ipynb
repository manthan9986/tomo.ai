{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm initialization (not secured with env file )\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_aws import ChatBedrock\n",
    "from typing import Annotated\n",
    "from langchain.docstore.document import Document\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    documents: list[Document] \n",
    "    pdf_path:str \n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\",api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag tool \n",
    "import spacy,pickle,os,boto3\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from pdfminer.high_level import extract_text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "@tool\n",
    "def rag_tool(state: dict) -> None:\n",
    "    \n",
    "    path = state[\"pdf_path\"]\n",
    "    \n",
    "    def text_extracter(path):\n",
    "        text = extract_text(path)\n",
    "        return text\n",
    "\n",
    "    def text_chunk(text):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,separators = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "        chunks = splitter.split_text(text)\n",
    "        return chunks\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def text_cleaning(chunks):\n",
    "        cleaned_chunks = []\n",
    "        for chunk in chunks:\n",
    "            doc = nlp(chunk)\n",
    "            cleaned = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "            cleaned_chunks.append(cleaned)\n",
    "        return cleaned_chunks\n",
    "    \n",
    "    def lemmatize_text(cleaned_chunks):\n",
    "        lemma_chunks = []\n",
    "        for chunk in cleaned_chunks:\n",
    "            doc = nlp(chunk)\n",
    "            lemma = \" \".join([token.lemma_ for token in doc])\n",
    "            lemma_chunks.append(lemma)\n",
    "        return lemma_chunks\n",
    "    \n",
    "    def load_store():\n",
    "        storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "        with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        \n",
    "        vector_store = FAISS.load_local(storage_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "        return vector_store\n",
    "\n",
    "    def embed_store(lemma_chunks):\n",
    "        docs = [Document(page_content=chunk) for chunk in lemma_chunks]\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        db = FAISS.from_documents(docs, embeddings)  # FAISS - Facebook AI Similarity Search (full form for revision)\n",
    "        \n",
    "        storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "        os.makedirs(storage_dir, exist_ok=True)\n",
    "        db.save_local(storage_dir)\n",
    "        \n",
    "        with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        \n",
    "        return db\n",
    "    \n",
    "    def rag_pipeline(path): \n",
    "        extracted_text = text_extracter(path)\n",
    "        chunked_text = text_chunk(extracted_text)\n",
    "        cleaned_text = text_cleaning(chunked_text)\n",
    "        lemma_text = lemmatize_text(cleaned_text)\n",
    "        embed_store(lemma_text)\n",
    "        embed_text = load_store()\n",
    "        return embed_text\n",
    "    \n",
    "    def retrieve_data(path: str,state: dict) -> None:\n",
    "        vector_store = rag_pipeline(path)\n",
    "        state[\"documents\"] = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "        return state \n",
    "    \n",
    "    retrieve_data(path,state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state management and error handling \n",
    "\n",
    "def handle_too_error(state:dict):\n",
    "    error = state.get\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
