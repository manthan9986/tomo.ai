{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm initialization (not secured with env file )\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_aws import ChatBedrock\n",
    "from typing import Annotated\n",
    "from langchain.docstore.document import Document\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\",api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag tool \n",
    "import spacy,pickle,os,boto3\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from pdfminer.high_level import extract_text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "@tool\n",
    "def rag_tool(state: dict) -> None:\n",
    "    \n",
    "    path = state[\"pdf_path\"]\n",
    "    \n",
    "    def text_extracter(path):\n",
    "        text = extract_text(path)\n",
    "        return text\n",
    "\n",
    "    def text_chunk(text):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,separators = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "        chunks = splitter.split_text(text)\n",
    "        return chunks\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def text_cleaning(chunks):\n",
    "        cleaned_chunks = []\n",
    "        for chunk in chunks:\n",
    "            doc = nlp(chunk)\n",
    "            cleaned = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "            cleaned_chunks.append(cleaned)\n",
    "        return cleaned_chunks\n",
    "    \n",
    "    def lemmatize_text(cleaned_chunks):\n",
    "        lemma_chunks = []\n",
    "        for chunk in cleaned_chunks:\n",
    "            doc = nlp(chunk)\n",
    "            lemma = \" \".join([token.lemma_ for token in doc])\n",
    "            lemma_chunks.append(lemma)\n",
    "        return lemma_chunks\n",
    "    \n",
    "    def load_store():\n",
    "        storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "        with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        \n",
    "        vector_store = FAISS.load_local(storage_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "        return vector_store\n",
    "\n",
    "    def embed_store(lemma_chunks):\n",
    "        docs = [Document(page_content=chunk) for chunk in lemma_chunks]\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        db = FAISS.from_documents(docs, embeddings)  # FAISS - Facebook AI Similarity Search (full form for revision)\n",
    "        \n",
    "        storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "        os.makedirs(storage_dir, exist_ok=True)\n",
    "        db.save_local(storage_dir)\n",
    "        \n",
    "        with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        \n",
    "        return db\n",
    "    \n",
    "    def rag_pipeline(path): \n",
    "        extracted_text = text_extracter(path)\n",
    "        chunked_text = text_chunk(extracted_text)\n",
    "        cleaned_text = text_cleaning(chunked_text)\n",
    "        lemma_text = lemmatize_text(cleaned_text)\n",
    "        embed_store(lemma_text)\n",
    "        embed_text = load_store()\n",
    "        return embed_text\n",
    "    \n",
    "    def retrieve_data(path: str,state: dict) -> None:\n",
    "        vector_store = rag_pipeline(path)\n",
    "        state[\"documents\"] = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "        return state \n",
    "    \n",
    "    vector_store = retrieve_data(path,state)\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    documents: list[Document] \n",
    "    pdf_path:str \n",
    "    question: str\n",
    "    question_type:str\n",
    "    pdf_context: str\n",
    "    calender_context: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes building \n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "router_prompt = \"\"\"You are an AI question router. Your job is to classify incoming user questions to route them to the correct specialized agent.\n",
    "\n",
    "There are two types of questions:\n",
    "- PDF: Questions that involve reading, summarizing, or querying the content of a PDF document.\n",
    "- CALENDAR: Questions related to scheduling, managing events, or checking availability using Google Calendar.\n",
    "\n",
    "Only respond with one word: PDF or CALENDAR.\n",
    "\"\"\"\n",
    "\n",
    "pdf_prompt = \"\"\"You are an expert in reading and answering questions based on the content of a PDF document.\n",
    "\n",
    "Your goal is to:\n",
    "- Answer user questions using the content of the PDF\n",
    "- If no specific question is asked, summarize the key points of the document\n",
    "- Keep responses concise and relevant to the document\n",
    "\n",
    "Respond clearly and based only on the content provided.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def router_node(state:GraphState):\n",
    "    messages = [SystemMessage(content=router_prompt), HumanMessage(content=state[\"question\"])]\n",
    "    router_model = llm\n",
    "    response = router_model.invoke(messages)\n",
    "    return {\"question_type\" : response.content}\n",
    "\n",
    "def rag_node(state:GraphState):\n",
    "    vector_store = rag_tool(state[\"pdf_path\"], state)\n",
    "    retriever_tool = create_retriever_tool(vector_store, name=\"rag_tool\", description=\"A tool to retrieve relevant information from a PDF document.\") \n",
    "    rag_agent = create_react_agent(llm=llm,tools=[retriever_tool] )\n",
    "    messages = [SystemMessage(content=pdf_prompt),HumanMessage(content=state[\"question\"])]\n",
    "    response = rag_agent.invoke(messages)\n",
    "    return {\"pdf_context\": response.content} if response else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our graph \n",
    "from langgraph.graph.state import StateGraph\n",
    "\n",
    "tomo_ai = StateGraph(GraphState)\n",
    "tomo_ai.add_node(\"router node\", router_node)\n",
    "tomo_ai.add_node(\"rag node\", rag_node,)\n",
    "tomo_ai.add_conditional_edges()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
