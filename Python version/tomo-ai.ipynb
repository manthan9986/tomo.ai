{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_extracter(path):\n",
    "    text = extract_text(path)\n",
    "    return text\n",
    "\n",
    "def text_chunk(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,separators = [\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def text_cleaning(chunks):\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        cleaned = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "        cleaned_chunks.append(cleaned)\n",
    "    return cleaned_chunks\n",
    "\n",
    "def lemmatize_text(cleaned_chunks):\n",
    "    lemma_chunks = []\n",
    "    for chunk in cleaned_chunks:\n",
    "        doc = nlp(chunk)\n",
    "        lemma = \" \".join([token.lemma_ for token in doc])\n",
    "        lemma_chunks.append(lemma)\n",
    "    return lemma_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def embed_store(lemma_chunks):\n",
    "  docs = [Document(page_content=chunk) for chunk in lemma_chunks]\n",
    "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "  db = FAISS.from_documents(docs, embeddings)  # FAISS - Facebook ai similarity search (full form for revision)\n",
    "  \n",
    "  storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "  os.makedirs(storage_dir, exist_ok=True)\n",
    "  db.save_local(storage_dir)\n",
    "  \n",
    "  with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "    \n",
    "  return db\n",
    "\n",
    "\n",
    "def load_store():\n",
    "  storage_dir = \"/Users/pmanthan/Desktop/tomo.ai/faiss_storage\"\n",
    "  with open(\"/Users/pmanthan/Desktop/tomo.ai/faiss_storage/faiss_storage.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    \n",
    "    vector_store = FAISS.load_local(storage_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "    return vector_store\n",
    "  \n",
    "  return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def rag_pipeline(): \n",
    "   extracted_text = text_extracter(\"/Users/pmanthan/Desktop/attentionisalluneed.pdf\")\n",
    "   chunked_text = text_chunk(extracted_text)\n",
    "   cleaned_text = text_cleaning(chunked_text)\n",
    "   lemma_text = lemmatize_text(cleaned_text)\n",
    "   embed_text = embed_store(lemma_text)\n",
    "   return load_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"llama3-70b-8192\",api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = rag_pipeline()\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key=\"answer\")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriever,memory=memory,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the provided text appears to be a collection of fragments from a research paper, including tables, figures, and references. It does not form a coherent narrative, making it challenging to provide a concise summary.\n",
      "\n",
      "The text seems to be related to natural language processing, machine translation, and attention mechanisms in neural networks. There are snippets about model architectures, training regimes, and experimental results, but the context is lacking.\n",
      "\n",
      "If you could provide more context or clarify what specific aspects of the paper you would like me to summarize, I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "def hybrid_answer(question):\n",
    "    response = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "    if not response[\"source_documents\"]:\n",
    "        fallback_answer = llm.invoke(question)\n",
    "        return fallback_answer\n",
    "\n",
    "    return response[\"answer\"]\n",
    "\n",
    "def input_output(question):\n",
    "    hy_answer = hybrid_answer(question)\n",
    "    print(hy_answer)\n",
    "\n",
    "\n",
    "input_output(\"can u summarize the pdf for me\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
